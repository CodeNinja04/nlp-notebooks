{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentimental analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_XBJXFm776S",
        "outputId": "5faab7c1-cd7c-42d9-9f83-0c993eb33a97"
      },
      "source": [
        "# sentimental analysis\n",
        "\n",
        "#importing libraries\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from textblob import Word\n",
        "from nltk.util import ngrams\n",
        "import re\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "\n",
        "def preprocess(sent):\n",
        "  text=sent\n",
        "  text.lower()\n",
        "  \n",
        "  #Remove additional white spaces\n",
        "  #text = re.sub('[\\s]+', ' ', text)\n",
        "  #text = re.sub('[\\n]+', ' ', text)\n",
        "  #Remove not alphanumeric symbols white spaces\n",
        "  #text = re.sub(r'[^\\w]', ' ', text)\n",
        "  #remove numbers\n",
        "  text = \"\".join([i for i in text if not i.isdigit()])\n",
        "  #remove multiple exclamation\n",
        "  text = re.sub(r\"(\\!)\\1+\", ' ', text)\n",
        "  #remove multiple question marks\n",
        "  text = re.sub(r\"(\\?)\\1+\", ' ', text)\n",
        "  #remove multistop\n",
        "  text = re.sub(r\"(\\.)\\1+\", ' ', text)\n",
        "  #lemmatization\n",
        "  text =\" \".join([Word(word).lemmatize() for word in text.split()])\n",
        "  #stemmer\n",
        "  st = SnowballStemmer(language='english')\n",
        "  text=\" \".join([st.stem(word) for word in text.split()])\n",
        "  text = text.strip('\\'\"')\n",
        "\n",
        "  return text\n",
        "\n",
        "text1=\"I like this laptop. screen quality and camera clarity is really good.\"\n",
        "text2=\"This laptop is not good. Bad quality, no clarity, worstexperience\"\n",
        "\n",
        "\n",
        "\n",
        "new_text1=preprocess(text1)\n",
        "new_text2=preprocess(text2)\n",
        "print(new_text1)\n",
        "blob1=TextBlob(new_text1)\n",
        "print(blob1.sentiment)\n",
        "print(new_text2)\n",
        "blob = TextBlob(text2)\n",
        "print(blob.sentiment)\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "i like this laptop. screen qualiti and camera clariti is realli good.\n",
            "Sentiment(polarity=0.7, subjectivity=0.6000000000000001)\n",
            "this laptop is not good. bad quality, no clarity, worstexperi\n",
            "Sentiment(polarity=-0.5249999999999999, subjectivity=0.6333333333333333)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}